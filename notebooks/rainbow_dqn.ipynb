{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Atari Paper Upgrade using Rainbow\n",
    "Here we will upgrade the DQN Atari paper using the Rainbow algorithm.\n",
    "From the collection of improvements in the Rainbow algorithm, we will implement the following:\n",
    "- Dueling Network Architecture\n",
    "- Prioritized Experience Replay\n",
    "- N-Step Returns\n",
    "- Noisy Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install gymnasium[atari,accept-rom-license] torch numpy opencv-python matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "As per the paper, we use certain hyperparameters that were tuned across various Atari games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x13390b3b0>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LEARNING_RATE = 0.0002\n",
    "DISCOUNT_FACTOR = 0.99\n",
    "REPLAY_MEMORY_SIZE = 150_000\n",
    "MINI_BATCH_SIZE = 32\n",
    "TARGET_UPDATE_FREQ = 1_200\n",
    "FRAME_SKIP = 4\n",
    "MIN_EPSILON = 0.1\n",
    "MAX_EPSILON = 1.0\n",
    "EPSILON_PHASE = 0.1\n",
    "MAX_STEPS = 1_500_001\n",
    "REPLAY_START_SIZE = 75_000\n",
    "SAVE_FREQUENCY = 500_000\n",
    "\n",
    "N_STEP = 3  # For N-Step Returns\n",
    "ALPHA = 0.6  # Prioritization exponent\n",
    "BETA_START = 0.4  # Initial value of beta for importance sampling\n",
    "BETA_FRAMES = MAX_STEPS - REPLAY_START_SIZE  # Schedule for beta\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prioritized Replay Buffer\n",
    "The paper introduces Prioritized Replay Buffer to sample important transitions more frequently. We use a SumTree data structure to store priorities and sample transitions based on the priorities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity, alpha, obs_shape):\n",
    "        self.capacity = capacity\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.pos = 0\n",
    "        self.size = 0\n",
    "\n",
    "        # Pre-allocate memory for buffer components\n",
    "        self.states = np.empty((capacity, *obs_shape), dtype=np.uint8)\n",
    "        self.next_states = np.empty((capacity, *obs_shape), dtype=np.uint8)\n",
    "        self.actions = np.empty((capacity,), dtype=np.int32)\n",
    "        self.rewards = np.empty((capacity,), dtype=np.float32)\n",
    "        self.dones = np.empty((capacity,), dtype=np.bool_)\n",
    "\n",
    "        self.priorities = np.zeros((capacity,), dtype=np.float32)\n",
    "        self.max_priority = 1.0\n",
    "\n",
    "    def add(self, error, state, next_state, action, reward, done):\n",
    "        self.states[self.pos] = state\n",
    "        self.next_states[self.pos] = next_state\n",
    "        self.actions[self.pos] = action\n",
    "        self.rewards[self.pos] = reward\n",
    "        self.dones[self.pos] = done\n",
    "\n",
    "        self.priorities[self.pos] = self.max_priority ** self.alpha\n",
    "\n",
    "        self.pos = (self.pos + 1) % self.capacity\n",
    "        self.size = min(self.size + 1, self.capacity)\n",
    "\n",
    "    def sample(self, batch_size, beta):\n",
    "        if self.size == self.capacity:\n",
    "            prios = self.priorities\n",
    "        else:\n",
    "            prios = self.priorities[:self.size]\n",
    "\n",
    "        probs = prios / prios.sum()\n",
    "        indices = np.random.choice(self.size, batch_size, p=probs)\n",
    "\n",
    "        # Retrieve samples directly without using zip()\n",
    "        states = self.states[indices]\n",
    "        next_states = self.next_states[indices]\n",
    "        actions = self.actions[indices]\n",
    "        rewards = self.rewards[indices]\n",
    "        dones = self.dones[indices]\n",
    "\n",
    "        total = self.size\n",
    "        weights = (total * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "        weights = weights.astype(np.float32)\n",
    "\n",
    "        return states, next_states, actions, rewards, dones, indices, weights\n",
    "\n",
    "    def update(self, idxs, errors):\n",
    "        errors = np.abs(errors) + 1e-6\n",
    "        self.max_priority = max(self.max_priority, errors.max())\n",
    "        self.priorities[idxs] = errors ** self.alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noisy Linear Layers\n",
    "The paper introduces Noisy Linear Layers to add noise to the weights of the linear layers. We use a NoisyLinear layer to add noise to the weights of the linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NoisyLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, std_init=0.5):\n",
    "        super(NoisyLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.std_init = std_init\n",
    "\n",
    "        self.weight_mu = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        self.weight_sigma = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        self.register_buffer(\n",
    "            \"weight_epsilon\", torch.FloatTensor(out_features, in_features)\n",
    "        )\n",
    "\n",
    "        self.bias_mu = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        self.bias_sigma = nn.Parameter(torch.FloatTensor(out_features))\n",
    "        self.register_buffer(\"bias_epsilon\", torch.FloatTensor(out_features))\n",
    "\n",
    "        self.reset_parameters()\n",
    "        self.reset_noise()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        mu_range = 1 / np.sqrt(self.weight_mu.size(1))\n",
    "        self.weight_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.weight_sigma.data.fill_(\n",
    "            self.std_init / np.sqrt(self.weight_sigma.size(1))\n",
    "        )\n",
    "\n",
    "        self.bias_mu.data.uniform_(-mu_range, mu_range)\n",
    "        self.bias_sigma.data.fill_(self.std_init / np.sqrt(self.bias_sigma.size(0)))\n",
    "\n",
    "    def reset_noise(self):\n",
    "        epsilon_in = self._scale_noise(self.in_features).to(device)\n",
    "        epsilon_out = self._scale_noise(self.out_features).to(device)\n",
    "\n",
    "        self.weight_epsilon.copy_(epsilon_out.ger(epsilon_in))\n",
    "        self.bias_epsilon.copy_(epsilon_out)\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.training:\n",
    "            weight = (\n",
    "                self.weight_mu + self.weight_sigma * self.weight_epsilon\n",
    "            )\n",
    "            bias = self.bias_mu + self.bias_sigma * self.bias_epsilon\n",
    "        else:\n",
    "            weight = self.weight_mu\n",
    "            bias = self.bias_mu\n",
    "\n",
    "        return F.linear(input, weight, bias)\n",
    "\n",
    "    def _scale_noise(self, size):\n",
    "        x = torch.randn(size)\n",
    "        return x.sign() * x.abs().sqrt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Network Architecture\n",
    "Dueling Network Architecture is used in the Rainbow algorithm. The architecture consists of two streams, one for the state value and the other for the advantage values. The two streams are combined to produce the Q-values.\n",
    "\n",
    "We will also use the previously implemented NoisyLinear layer to add noise to the weights of the linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetwork(nn.Module):\n",
    "    def __init__(self, n_actions):\n",
    "        super(DeepQNetwork, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        self.n_actions = n_actions\n",
    "        self.fc_input_dim = 7 * 7 * 64\n",
    "\n",
    "        # Dueling DQN streams with NoisyLinear layers\n",
    "        self.value_stream = nn.Sequential(\n",
    "            NoisyLinear(self.fc_input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            NoisyLinear(512, 1),\n",
    "        )\n",
    "\n",
    "        self.advantage_stream = nn.Sequential(\n",
    "            NoisyLinear(self.fc_input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            NoisyLinear(512, n_actions),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x / 255.0)\n",
    "        value = self.value_stream(x)\n",
    "        advantage = self.advantage_stream(x)\n",
    "        q_vals = value + (advantage - advantage.mean(dim=1, keepdim=True))\n",
    "        return q_vals\n",
    "\n",
    "    def reset_noise(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, NoisyLinear):\n",
    "                m.reset_noise()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Selection\n",
    "As in the paper, we use an epsilon-greedy policy to select actions during training. We start with a high epsilon value and decay it over time. In addition to the NoisyLinear layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gymnasium Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we set up the gym environment, by selecting the Breakout game. We specify RMSProp as the optimizer just like in the paper training details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_id, render_mode=None, frame_skip=4):\n",
    "    \"\"\"Create environment with preprocessing wrappers.\"\"\"\n",
    "    env = gym.make(env_id, render_mode=render_mode, frameskip=1)\n",
    "    env = gym.wrappers.AtariPreprocessing(env, frame_skip=4)\n",
    "    env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "    env = gym.wrappers.FrameStack(env, 4)\n",
    "    env = gym.wrappers.AutoResetWrapper(env)\n",
    "    return env\n",
    "\n",
    "\n",
    "env = make_env(\"ALE/Breakout-v5\")\n",
    "\n",
    "n_actions = env.action_space.n\n",
    "dqn = DeepQNetwork(n_actions).to(device)\n",
    "optimizer = torch.optim.Adam(dqn.parameters(), lr=LEARNING_RATE)\n",
    "dqn_prime = DeepQNetwork(n_actions).to(device)\n",
    "\n",
    "buffer = PrioritizedReplayBuffer(\n",
    "    REPLAY_MEMORY_SIZE, alpha=ALPHA, obs_shape=(4, 84, 84)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 6,507,690\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def count_trainable_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total_params\n",
    "\n",
    "print(f\"Trainable parameters: {count_trainable_parameters(dqn):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepQNetwork(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (5): ReLU()\n",
       "    (6): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (value_stream): Sequential(\n",
       "    (0): NoisyLinear()\n",
       "    (1): ReLU()\n",
       "    (2): NoisyLinear()\n",
       "  )\n",
       "  (advantage_stream): Sequential(\n",
       "    (0): NoisyLinear()\n",
       "    (1): ReLU()\n",
       "    (2): NoisyLinear()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "Here, we follow the methodology from the paper, by putting all the above components together to train the DQN agent on the Atari game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 0/1500001 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R: 2.00, l: 0.00, Mean Q: -0.01, e: 0.80:   2%|â–         | 32623/1500001 [00:38<29:11, 837.85it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 32\u001b[0m         \u001b[43mdqn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_noise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m         obs_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[1;32m     34\u001b[0m             np\u001b[38;5;241m.\u001b[39marray(t_observation), device\u001b[38;5;241m=\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32\n\u001b[1;32m     35\u001b[0m         )\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     36\u001b[0m         q_values \u001b[38;5;241m=\u001b[39m dqn(obs_tensor)\n",
      "Cell \u001b[0;32mIn[63], line 40\u001b[0m, in \u001b[0;36mDeepQNetwork.reset_noise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodules():\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(m, NoisyLinear):\n\u001b[0;32m---> 40\u001b[0m         \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_noise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[62], line 32\u001b[0m, in \u001b[0;36mNoisyLinear.reset_noise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset_noise\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 32\u001b[0m     epsilon_in \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_scale_noise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_features\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     33\u001b[0m     epsilon_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_scale_noise(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_features)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_epsilon\u001b[38;5;241m.\u001b[39mcopy_(epsilon_out\u001b[38;5;241m.\u001b[39mger(epsilon_in))\n",
      "Cell \u001b[0;32mIn[62], line 52\u001b[0m, in \u001b[0;36mNoisyLinear._scale_noise\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_scale_noise\u001b[39m(\u001b[38;5;28mself\u001b[39m, size):\n\u001b[1;32m     51\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(size)\n\u001b[0;32m---> 52\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39msign() \u001b[38;5;241m*\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabs\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_history = {\n",
    "    \"loss\": [0],\n",
    "    \"mean_q_value\": [0],\n",
    "    \"episode_rewards\": [0],\n",
    "    \"steps\": [0],\n",
    "}\n",
    "\n",
    "t_observation, _ = env.reset()\n",
    "episode_reward = 0\n",
    "\n",
    "progress_bar = tqdm(range(MAX_STEPS), desc=\"Training Progress\")\n",
    "\n",
    "episode_steps = 0\n",
    "episode_loss = 0\n",
    "episode_q_values = 0\n",
    "\n",
    "n_step_buffer = []\n",
    "\n",
    "for t in progress_bar:\n",
    "    # Epsilon with linear decay\n",
    "    eps = max(\n",
    "        MIN_EPSILON,\n",
    "        MIN_EPSILON\n",
    "        + (MAX_EPSILON - MIN_EPSILON) * (1 - t / (EPSILON_PHASE * MAX_STEPS)),\n",
    "    )\n",
    "\n",
    "    # Epsilon-greedy policy with Noisy Networks\n",
    "    if np.random.rand() < eps:\n",
    "        action = env.action_space.sample()\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            dqn.reset_noise()\n",
    "            obs_tensor = torch.tensor(\n",
    "                np.array(t_observation), device=device, dtype=torch.float32\n",
    "            ).unsqueeze(0)\n",
    "            q_values = dqn(obs_tensor)\n",
    "            action = torch.argmax(q_values, dim=1).item()\n",
    "            episode_q_values += q_values.mean().item()\n",
    "\n",
    "    # Step environment\n",
    "    t1_observation, reward, done, _, info = env.step(action)\n",
    "    episode_reward += reward\n",
    "\n",
    "    # Add to n-step buffer\n",
    "    n_step_buffer.append((t_observation, action, reward, done, t1_observation))\n",
    "\n",
    "    if len(n_step_buffer) >= N_STEP:\n",
    "        # Compute N-step return\n",
    "        R = sum(\n",
    "            [\n",
    "                n_step_buffer[i][2] * (DISCOUNT_FACTOR ** i)\n",
    "                for i in range(N_STEP)\n",
    "            ]\n",
    "        )\n",
    "        state = n_step_buffer[0][0]\n",
    "        action_n = n_step_buffer[0][1]\n",
    "        next_state = n_step_buffer[-1][4]\n",
    "        done_n = n_step_buffer[-1][3]\n",
    "\n",
    "        buffer.add(\n",
    "            error=buffer.max_priority,\n",
    "            state=state,\n",
    "            next_state=next_state,\n",
    "            action=action_n,\n",
    "            reward=R,\n",
    "            done=done_n,\n",
    "        )\n",
    "\n",
    "        n_step_buffer.pop(0)\n",
    "\n",
    "    if done:\n",
    "        # Add remaining transitions in n-step buffer\n",
    "        while len(n_step_buffer) > 0:\n",
    "            len_buffer = len(n_step_buffer)\n",
    "            R = sum(\n",
    "                [\n",
    "                    n_step_buffer[i][2] * (DISCOUNT_FACTOR ** i)\n",
    "                    for i in range(len_buffer)\n",
    "                ]\n",
    "            )\n",
    "            state = n_step_buffer[0][0]\n",
    "            action_n = n_step_buffer[0][1]\n",
    "            next_state = n_step_buffer[-1][4]\n",
    "            done_n = n_step_buffer[-1][3]\n",
    "\n",
    "            buffer.add(\n",
    "                error=buffer.max_priority,\n",
    "                state=state,\n",
    "                next_state=next_state,\n",
    "                action=action_n,\n",
    "                reward=R,\n",
    "                done=done_n,\n",
    "            )\n",
    "\n",
    "            n_step_buffer.pop(0)\n",
    "\n",
    "        training_history[\"steps\"].append(t)\n",
    "        training_history[\"episode_rewards\"].append(episode_reward)\n",
    "        training_history[\"mean_q_value\"].append(\n",
    "            episode_q_values / episode_steps if episode_steps > 0 else 0\n",
    "        )\n",
    "        training_history[\"loss\"].append(\n",
    "            episode_loss / episode_steps if episode_steps > 0 else 0\n",
    "        )\n",
    "        episode_reward = 0\n",
    "        progress_bar.set_description(\n",
    "            f\"R: {training_history['episode_rewards'][-1]:.2f}, l: {training_history['loss'][-1]:.2f}, Mean Q: {training_history['mean_q_value'][-1]:.2f}, e: {eps:.2f}\"\n",
    "        )\n",
    "        episode_steps = 0\n",
    "        episode_loss = 0\n",
    "        episode_q_values = 0\n",
    "\n",
    "        t_observation, _ = env.reset()\n",
    "    else:\n",
    "        t_observation = t1_observation\n",
    "\n",
    "    # Checkpoint every SAVE_FREQUENCY steps\n",
    "    if t > 0 and t % SAVE_FREQUENCY == 0:\n",
    "        torch.save(dqn.state_dict(), f\"checkpoint{t}.pt\")\n",
    "\n",
    "    if t > REPLAY_START_SIZE:\n",
    "        if t % 4 == 0:\n",
    "            beta = min(\n",
    "                1.0,\n",
    "                BETA_START + (t - REPLAY_START_SIZE) * (1.0 - BETA_START) / BETA_FRAMES,\n",
    "            )\n",
    "\n",
    "            (\n",
    "                states,\n",
    "                next_states,\n",
    "                actions,\n",
    "                rewards,\n",
    "                dones,\n",
    "                idxs,\n",
    "                is_weights,\n",
    "            ) = buffer.sample(MINI_BATCH_SIZE, beta)\n",
    "\n",
    "            # Convert to tensors\n",
    "            states = torch.tensor(states, device=device, dtype=torch.float32)\n",
    "            next_states = torch.tensor(next_states, device=device, dtype=torch.float32)\n",
    "            actions = torch.tensor(actions, device=device, dtype=torch.long)\n",
    "            rewards = torch.tensor(rewards, device=device, dtype=torch.float32)\n",
    "            dones = torch.tensor(dones.astype(np.float32), device=device)\n",
    "\n",
    "            is_weights = torch.tensor(is_weights, device=device, dtype=torch.float32)\n",
    "\n",
    "            # Reset noise in the networks\n",
    "            dqn.reset_noise()\n",
    "            dqn_prime.reset_noise()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                not_done = 1.0 - dones\n",
    "                # Double DQN with N-Step Returns\n",
    "                next_q_values = dqn_prime(next_states)\n",
    "                next_actions = dqn(next_states).argmax(dim=1)\n",
    "                next_q_values = next_q_values.gather(\n",
    "                    1, next_actions.unsqueeze(1)\n",
    "                ).squeeze(1)\n",
    "                y_j = rewards + (DISCOUNT_FACTOR ** N_STEP) * next_q_values * not_done\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            q_values = dqn(states)\n",
    "            q_values = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "            td_errors = y_j - q_values\n",
    "            loss = (is_weights * td_errors.pow(2)).mean()\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            episode_loss += loss.item()\n",
    "\n",
    "            # Update priorities in the buffer\n",
    "            errors = td_errors.detach().cpu().numpy()\n",
    "            buffer.update(idxs, errors)\n",
    "\n",
    "        if t % TARGET_UPDATE_FREQ == 0:\n",
    "            dqn_prime = deepcopy(dqn)\n",
    "\n",
    "    episode_steps += 1\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*The trainings were run on Kaggle, so the training logs are not included in this notebook.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "df_plot_infos = pd.DataFrame(training_history)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_plot_infos.to_csv(\"../data/rainbow_dqn_training_history.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "*The tests were run on Kaggle, so the tests logs are not included in this notebook.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mv/3fqdyl697kg4mqs3tkmd7gs80000gn/T/ipykernel_30026/196103572.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(checkpoint_file, map_location=device))\n",
      "/var/folders/mv/3fqdyl697kg4mqs3tkmd7gs80000gn/T/ipykernel_30026/196103572.py:11: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /Users/runner/miniforge3/conda-bld/libtorch_1724557170823/work/torch/csrc/utils/tensor_new.cpp:281.)\n",
      "  torch.tensor(obs, dtype=torch.float32).unsqueeze(0).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 1, Reward: 22.0\n",
      "Game 2, Reward: 13.0\n",
      "Game 3, Reward: 23.0\n",
      "Game 4, Reward: 11.0\n",
      "Game 5, Reward: 15.0\n",
      "Game 6, Reward: 17.0\n",
      "Game 7, Reward: 15.0\n",
      "Game 8, Reward: 23.0\n",
      "Game 9, Reward: 23.0\n",
      "Game 10, Reward: 20.0\n",
      "Game 11, Reward: 15.0\n",
      "Game 12, Reward: 12.0\n",
      "Game 13, Reward: 25.0\n",
      "Game 14, Reward: 22.0\n",
      "Game 15, Reward: 23.0\n",
      "Game 16, Reward: 38.0\n",
      "Game 17, Reward: 30.0\n",
      "Game 18, Reward: 30.0\n",
      "Game 19, Reward: 37.0\n",
      "Game 20, Reward: 21.0\n",
      "Game 21, Reward: 21.0\n",
      "Game 22, Reward: 19.0\n",
      "Game 23, Reward: 12.0\n",
      "Game 24, Reward: 33.0\n",
      "Game 25, Reward: 27.0\n",
      "Game 26, Reward: 11.0\n",
      "Game 27, Reward: 12.0\n",
      "Game 28, Reward: 16.0\n",
      "Game 29, Reward: 22.0\n",
      "Game 30, Reward: 10.0\n",
      "Game 31, Reward: 35.0\n",
      "Game 32, Reward: 16.0\n",
      "Game 33, Reward: 15.0\n",
      "Game 34, Reward: 29.0\n",
      "Game 35, Reward: 25.0\n",
      "Game 36, Reward: 31.0\n",
      "Game 37, Reward: 9.0\n",
      "Game 38, Reward: 19.0\n",
      "Game 39, Reward: 17.0\n",
      "Game 40, Reward: 12.0\n",
      "Game 41, Reward: 23.0\n",
      "Game 42, Reward: 11.0\n",
      "Game 43, Reward: 21.0\n",
      "Game 44, Reward: 23.0\n",
      "Game 45, Reward: 32.0\n",
      "Game 46, Reward: 16.0\n",
      "Game 47, Reward: 19.0\n",
      "Game 48, Reward: 29.0\n",
      "Game 49, Reward: 19.0\n",
      "Game 50, Reward: 15.0\n",
      "Average Reward: 20.68\n",
      "Standard Deviation: 7.406591658786112\n",
      "Max Reward: 38.0\n",
      "Min Reward: 9.0\n"
     ]
    }
   ],
   "source": [
    "# Function to load model weights from checkpoint file\n",
    "def load_checkpoint(model, checkpoint_file):\n",
    "    model.load_state_dict(torch.load(checkpoint_file, map_location=device))\n",
    "    model.eval()  # Set the model to evaluation mode (important for inference)\n",
    "\n",
    "\n",
    "# Function to play a single episode and return the total reward\n",
    "def play_episode(env, model):\n",
    "    obs, info = env.reset()\n",
    "    state = (\n",
    "        torch.tensor(obs, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    )  # Move to correct device\n",
    "    total_reward = 0\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        if np.random.rand() < 0.05:\n",
    "            action = env.action_space.sample()  # Random action with 5% probability\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                action = (\n",
    "                    model(state).argmax(dim=1).item()\n",
    "                )  # Choose action with highest Q-value\n",
    "        next_obs, reward, done, truncated, info = env.step(action)\n",
    "        next_state = torch.tensor(next_obs, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "\n",
    "        if done or truncated:\n",
    "            break\n",
    "\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "# Function to evaluate the model by playing 50 games\n",
    "def evaluate_model(checkpoint_file, num_games=50):\n",
    "    # Create the environment\n",
    "    env = make_env(\"ALE/Breakout-v5\", frame_skip=4)\n",
    "\n",
    "    # Initialize model\n",
    "    action_space = env.action_space.n\n",
    "    model = DeepQNetwork(action_space).to(device)\n",
    "\n",
    "    # Load the best checkpoint\n",
    "    load_checkpoint(model, checkpoint_file)\n",
    "\n",
    "    total_rewards = []\n",
    "    for game in range(num_games):\n",
    "        total_reward = play_episode(env, model)\n",
    "        total_rewards.append(total_reward)\n",
    "        print(f\"Game {game + 1}, Reward: {total_reward}\")\n",
    "\n",
    "    # Calculate average reward\n",
    "    avg_reward = np.mean(total_rewards)\n",
    "    std_reward = np.std(total_rewards)\n",
    "    max_reward = np.max(total_rewards)\n",
    "    min_reward = np.min(total_rewards)\n",
    "\n",
    "    print(f\"Average Reward: {avg_reward}\")\n",
    "    print(f\"Standard Deviation: {std_reward}\")\n",
    "    print(f\"Max Reward: {max_reward}\")\n",
    "    print(f\"Min Reward: {min_reward}\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "\n",
    "# Call the function to evaluate the model\n",
    "best_checkpoint_path = \"checkpoint1500000.pth\"\n",
    "evaluate_model(best_checkpoint_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research-rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
