{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Atari Paper Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install gymnasium[atari,accept-rom-license] torch numpy opencv-python matplotlib torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "As per the paper, we use certain hyperparameters that were tuned across various Atari games."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 0.00025  # Paper used a similar learning rate\n",
    "DISCOUNT_FACTOR = 0.99  # The γ discount factor as mentioned in the paper\n",
    "REPLAY_MEMORY_SIZE = 150_000  # Large replay buffer as described, but not too large\n",
    "BATCH_SIZE = 32  # Minibatch size for training\n",
    "TARGET_UPDATE_FREQ = 1_250  # C steps for target network update\n",
    "FRAME_SKIP = 4  # Number of frames skipped\n",
    "MIN_EPSILON = 0.1  # Minimum value of epsilon (for more exploitation)\n",
    "MAX_EPSILON = 1.0  # Starting value of epsilon (for exploration)\n",
    "EPSILON_PHASE = 0.1 # Percentage of steps for epsilon to reach MIN_EPSILON\n",
    "MAX_STEPS = 1_650_000  # Total training episodes\n",
    "REPLAY_START_SIZE = 50_000  # Size of replay memory before starting training\n",
    "SAVE_FREQUENCY = 50_000  # Save model every 50k steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Network Architecture\n",
    "Referring to the paper's architecture (3 convolutional layers, followed by fully connected layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(torch.nn.Module):\n",
    "    def __init__(self, n_actions):\n",
    "        super(DQN, self).__init__()\n",
    "        self.conv = torch.nn.Sequential(\n",
    "            # Convolution layers (as per paper), input: 84x84x4 image\n",
    "            torch.nn.Conv2d(4, 32, kernel_size=8, stride=4),\n",
    "            # Using ReLU activations as specified in the paper\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Flatten(),\n",
    "        )\n",
    "\n",
    "        self.linear = torch.nn.Sequential(\n",
    "            torch.nn.Linear(7 *  7 * 64 , 512),\n",
    "            torch.nn.ReLU(),\n",
    "            # Final fully connected layer with one output for each action\n",
    "            torch.nn.Linear(512, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Wrappers\n",
    "We use custom wrappers using gymnasium's API.\n",
    "The paper describes preprocessing the input frames by converting them to grayscale, resizing, normalizing and stacking the last 4 frames to capture motion. We will use the same preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FrameSkip(gym.Wrapper):\n",
    "    def __init__(self, env, skip=4):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super(FrameSkip, self).__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, sum reward, and take the last observation.\"\"\"\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for _ in range(self._skip):\n",
    "            obs, reward, done, truncated, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done or truncated:\n",
    "                break\n",
    "        return obs, total_reward, done, truncated, info\n",
    "\n",
    "\n",
    "class GrayScaleResize(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        \"\"\"Convert the observations to grayscale and resize to 84x84.\"\"\"\n",
    "        super(GrayScaleResize, self).__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0, high=255, shape=(84, 84), dtype=np.uint8\n",
    "        )\n",
    "\n",
    "    def observation(self, obs):\n",
    "        obs = cv2.cvtColor(obs, cv2.COLOR_RGB2GRAY)  # Convert to grayscale\n",
    "        obs = cv2.resize(obs, (84, 84), interpolation=cv2.INTER_AREA)  # Resize to 84x84\n",
    "        return obs\n",
    "\n",
    "\n",
    "class FrameStack(gym.Wrapper):\n",
    "    def __init__(self, env, k=4):\n",
    "        \"\"\"Stack `k` last frames.\"\"\"\n",
    "        super(FrameStack, self).__init__(env)\n",
    "        self.k = k\n",
    "        self.frames = deque([], maxlen=k)\n",
    "        shp = env.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0, high=255, shape=(k, shp[0], shp[1]), dtype=np.uint8\n",
    "        )\n",
    "\n",
    "    def reset(self):\n",
    "        obs, info = self.env.reset()\n",
    "        for _ in range(self.k):\n",
    "            self.frames.append(obs)\n",
    "        return self._get_ob(), info\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, done, truncated, info = self.env.step(action)\n",
    "        self.frames.append(obs)\n",
    "        return self._get_ob(), reward, done, truncated, info\n",
    "\n",
    "    def _get_ob(self):\n",
    "        return np.array(self.frames)\n",
    "\n",
    "\n",
    "class NormalizeObs(gym.ObservationWrapper):\n",
    "    \"\"\"Normalize observations to the range [0, 1].\"\"\"\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super(NormalizeObs, self).__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=0, high=1.0, shape=env.observation_space.shape, dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def observation(self, obs):\n",
    "        return np.array(obs).astype(np.float32) / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Memory\n",
    "We implement an experience replay buffer as described in the paper to store past experiences and sample them randomly during training to break the correlation between consecutive frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, size, obs_shape, action_shape):\n",
    "        self.size = size\n",
    "        self.obs_shape = obs_shape\n",
    "        self.action_shape = action_shape\n",
    "\n",
    "        self.t_obs = np.empty((size, *obs_shape), dtype=np.uint8)\n",
    "        self.t1_obs = np.empty((size, *obs_shape), dtype=np.uint8)\n",
    "        self.actions = np.empty((size, *action_shape), dtype=np.uint8)\n",
    "        self.rewards = np.empty(size, dtype=np.float16)\n",
    "        self.dones = np.empty(size, dtype=np.bool_)\n",
    "\n",
    "        self.idx = 0\n",
    "        self.current_size = 0\n",
    "\n",
    "    def append(self, t_obs, t1_obs, actions, reward, done):\n",
    "        self.t_obs[self.idx] = t_obs\n",
    "        self.t1_obs[self.idx] = t1_obs\n",
    "        self.actions[self.idx] = actions\n",
    "        self.rewards[self.idx] = reward\n",
    "        self.dones[self.idx] = done\n",
    "\n",
    "        self.current_size = min(self.current_size + 1, self.size)\n",
    "        self.idx = (self.idx + 1) % self.size\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        ids = np.random.choice(self.current_size, batch_size, replace=False)\n",
    "        batch = (\n",
    "            self.t_obs[ids],\n",
    "            self.t1_obs[ids],\n",
    "            self.actions[ids],\n",
    "            self.rewards[ids],\n",
    "            self.dones[ids],\n",
    "        )\n",
    "\n",
    "        return tuple(\n",
    "            torch.as_tensor(item, dtype=torch.float32).to(device) for item in batch\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.current_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Epsilon-greedy action selection\n",
    "This function selects an action using epsilon-greedy policy as mentioned in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(state, policy_net, epsilon, action_space):\n",
    "    if random.random() < epsilon:  # Explore\n",
    "        return random.choice(action_space)\n",
    "    else:  # Exploit\n",
    "        with torch.no_grad():\n",
    "            state = torch.tensor(state, dtype=torch.float32).to(device).unsqueeze(0)\n",
    "            return policy_net(state).argmax(dim=1).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Network Update\n",
    "Target network helps stabilize learning. The paper mentions that the target network is updated every C steps.\n",
    "Unlike the paper here we will use a Double DQN approach where the target network is updated with the weights of the main network every C steps. This helps in reducing overestimation of Q-values and training time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target(policy_net, target_net):\n",
    "    target_net.load_state_dict(policy_net.state_dict())  # Sync the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gymnasium Environment Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we set up the gym environment, by selecting the Breakout game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "def make_env(env_id, render_mode=None, frame_skip=4):\n",
    "    \"\"\"Create environment with preprocessing wrappers.\"\"\"\n",
    "    env = gym.make(env_id, render_mode=render_mode, frameskip=1)\n",
    "    env = FrameSkip(env, skip=frame_skip)  # Frame skipping (skip 4 frames)\n",
    "    env = GrayScaleResize(env)  # Convert frames to grayscale and resize to 84x84\n",
    "    env = NormalizeObs(env)  # Normalize pixel values to [0, 1]\n",
    "    env = FrameStack(env, k=4)  # Stack the last 4 frames\n",
    "    return env\n",
    "\n",
    "\n",
    "env = make_env(\"ALE/Breakout-v5\")\n",
    "action_space = [i for i in range(env.action_space.n)]\n",
    "\n",
    "# Initialize policy and target networks\n",
    "policy_net = DQN(len(action_space)).to(device)\n",
    "target_net = DQN(len(action_space)).to(device)\n",
    "update_target(policy_net, target_net)  # First sync\n",
    "\n",
    "# Instead of RMSprop, we use Adam as the optimizer because we won't train\n",
    "# as much as the paper did\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=LEARNING_RATE)\n",
    "replay_buffer = ReplayBuffer(REPLAY_MEMORY_SIZE, (4, 84, 84), (1,))\n",
    "\n",
    "epsilon = MAX_EPSILON  # Starting value of epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 32, 20, 20]           8,224\n",
      "              ReLU-2           [-1, 32, 20, 20]               0\n",
      "            Conv2d-3             [-1, 64, 9, 9]          32,832\n",
      "              ReLU-4             [-1, 64, 9, 9]               0\n",
      "            Conv2d-5             [-1, 64, 7, 7]          36,928\n",
      "              ReLU-6             [-1, 64, 7, 7]               0\n",
      "           Flatten-7                 [-1, 3136]               0\n",
      "            Linear-8                  [-1, 512]       1,606,144\n",
      "              ReLU-9                  [-1, 512]               0\n",
      "           Linear-10                    [-1, 4]           2,052\n",
      "================================================================\n",
      "Total params: 1,686,180\n",
      "Trainable params: 1,686,180\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.11\n",
      "Forward/backward pass size (MB): 0.35\n",
      "Params size (MB): 6.43\n",
      "Estimated Total Size (MB): 6.89\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "summary(policy_net.to(\"cpu\"), input_size=(4, 84, 84), device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQN(\n",
       "  (conv): Sequential(\n",
       "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (5): ReLU()\n",
       "    (6): Flatten(start_dim=1, end_dim=-1)\n",
       "  )\n",
       "  (linear): Sequential(\n",
       "    (0): Linear(in_features=3136, out_features=512, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=512, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Reset the environment to get the initial state\n",
    "initial_state = env.reset()\n",
    "\n",
    "# Extract the first frame from the stacked frames\n",
    "first_frame = initial_state[0]\n",
    "\n",
    "# Plot the first frame using matplotlib\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(first_frame[0])\n",
    "plt.title(\"Initial Frame from Game\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "Here, we follow the methodology from the paper, by putting all the above components together to train the DQN agent on the Atari game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 0/500000 [00:00<?, ?it/s]2024-10-01 20:20:03.755 python[90984:2631555] +[IMKClient subclass]: chose IMKClient_Legacy\n",
      "2024-10-01 20:20:03.755 python[90984:2631555] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n",
      "R: 1.0, ε: 0.99667, RSize: 185 Q-value: 0.00000, Loss: 0.00000:   0%|          | 185/500000 [00:07<5:50:01, 23.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Reward: 1.0\n"
     ]
    }
   ],
   "source": [
    "total_steps = 0\n",
    "\n",
    "plot_infos = {\n",
    "    \"total_steps\": [],\n",
    "    \"total_reward\": [],\n",
    "    \"epsilon\": [],\n",
    "    \"total_q_values\": [],\n",
    "    \"total_loss\": [],\n",
    "}\n",
    "\n",
    "progress_bar = tqdm(range(MAX_STEPS), desc=\"Training Progress\")\n",
    "\n",
    "while total_steps < MAX_STEPS:\n",
    "    state, info = env.reset()  # Reset environment to initial state\n",
    "\n",
    "    total_reward = 0\n",
    "    total_q_values = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    while True:\n",
    "        action = select_action(state, policy_net, epsilon, action_space)\n",
    "\n",
    "        # Step the environment\n",
    "        next_state, reward, done, truncated, _ = env.step(action)\n",
    "\n",
    "        # Clip the reward to be in the range [-1, 1] as mentioned in the paper\n",
    "        reward = np.sign(reward)\n",
    "\n",
    "        # Store the transition in replay memory\n",
    "        replay_buffer.append(state, next_state, action, reward, done)\n",
    "\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        total_steps += 1\n",
    "\n",
    "        # Only start training when replay memory has enough samples\n",
    "        if len(replay_buffer) >= REPLAY_START_SIZE:\n",
    "            if total_steps % 4 == 0: # Update every 4 steps like in the paper\n",
    "                # Sample minibatch from replay buffer\n",
    "                batch = replay_buffer.sample(BATCH_SIZE)\n",
    "\n",
    "                batch_state, batch_next_state, batch_action, batch_reward, batch_done = (\n",
    "                    batch\n",
    "                )\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Compute Q targets\n",
    "                q_values = policy_net(batch_state).gather(1, batch_action.long())\n",
    "                next_q_values = target_net(batch_next_state).max(1)[0].detach()\n",
    "                target_q_values = batch_reward + (\n",
    "                    DISCOUNT_FACTOR * next_q_values * (~batch_done.bool())\n",
    "                )\n",
    "\n",
    "                loss = torch.nn.functional.huber_loss(q_values, target_q_values.unsqueeze(1))\n",
    "\n",
    "                total_q_values += q_values.mean().item()\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                # Backpropagate and update the network\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # Update target network periodically\n",
    "            if total_steps % TARGET_UPDATE_FREQ == 0:\n",
    "                update_target(policy_net, target_net)\n",
    "\n",
    "            # Save checkpoints every SAVE_FREQUENCY steps\n",
    "            if total_steps % SAVE_FREQUENCY == 0:\n",
    "                torch.save(\n",
    "                    policy_net.state_dict(), f\"checkpoints/checkpoint_{total_steps}.pth\"\n",
    "                )\n",
    "\n",
    "        if done or truncated:\n",
    "            break\n",
    "    \n",
    "    # Append the total reward for tracking\n",
    "    # Decay epsilon for exploration-exploitation tradeoff\n",
    "    epsilon = max(MIN_EPSILON, epsilon - (MAX_EPSILON - MIN_EPSILON) * (total_steps / MAX_STEPS))\n",
    "    plot_infos[\"total_reward\"].append(total_reward)\n",
    "    plot_infos[\"epsilon\"].append(epsilon)\n",
    "    plot_infos[\"total_steps\"].append(total_steps)\n",
    "    plot_infos[\"total_q_values\"].append(total_q_values / total_steps)\n",
    "    plot_infos[\"total_loss\"].append(total_loss / total_steps)\n",
    "\n",
    "    progress_bar.set_description(\n",
    "        f\"R: {plot_infos['total_reward'][-1]}, ε: {plot_infos['epsilon'][-1]:.5f}, RSize: {len(replay_buffer)} Q-value: {plot_infos['total_q_values'][-1]:.5f}, Loss: {plot_infos['total_loss'][-1]:.5f}\"\n",
    "    )\n",
    "    progress_bar.update(plot_infos[\"total_steps\"][-1] - progress_bar.n)\n",
    "\n",
    "progress_bar.close()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the training progress of the DQN agent on the Breakout game. I created a plotting module outside of the notebook to plot the training progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_infos' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mplot_dict\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_dict\n\u001b[0;32m----> 3\u001b[0m plot_dict(\u001b[43mplot_infos\u001b[49m, REPLAY_START_SIZE)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_infos' is not defined"
     ]
    }
   ],
   "source": [
    "from plot_dict import plot_dict\n",
    "\n",
    "plot_dict(plot_infos, REPLAY_START_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "df_plot_infos = pd.DataFrame(plot_infos)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_plot_infos.to_csv('data/original_paper_plot_infos.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "Here is the training playing each 50000 steps."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research-rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
